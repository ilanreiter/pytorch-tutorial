{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step [100/50000], Loss: 2.1979, Acc: 0.47\n",
      "Step [200/50000], Loss: 2.1152, Acc: 0.64\n",
      "Step [300/50000], Loss: 1.9554, Acc: 0.70\n",
      "Step [400/50000], Loss: 1.8481, Acc: 0.73\n",
      "Step [500/50000], Loss: 1.6773, Acc: 0.75\n",
      "Step [600/50000], Loss: 1.5864, Acc: 0.72\n",
      "Step [700/50000], Loss: 1.4677, Acc: 0.77\n",
      "Step [800/50000], Loss: 1.3329, Acc: 0.79\n",
      "Step [900/50000], Loss: 1.2840, Acc: 0.73\n",
      "Step [1000/50000], Loss: 1.1341, Acc: 0.82\n",
      "Step [1100/50000], Loss: 1.1512, Acc: 0.77\n",
      "Step [1200/50000], Loss: 1.0849, Acc: 0.78\n",
      "Step [1300/50000], Loss: 0.9807, Acc: 0.84\n",
      "Step [1400/50000], Loss: 0.8671, Acc: 0.87\n",
      "Step [1500/50000], Loss: 0.8698, Acc: 0.85\n",
      "Step [1600/50000], Loss: 0.8506, Acc: 0.84\n",
      "Step [1700/50000], Loss: 0.9649, Acc: 0.79\n",
      "Step [1800/50000], Loss: 0.7497, Acc: 0.85\n",
      "Step [1900/50000], Loss: 0.7286, Acc: 0.88\n",
      "Step [2000/50000], Loss: 0.7093, Acc: 0.92\n",
      "Step [2100/50000], Loss: 0.5812, Acc: 0.86\n",
      "Step [2200/50000], Loss: 0.7738, Acc: 0.79\n",
      "Step [2300/50000], Loss: 0.6664, Acc: 0.88\n",
      "Step [2400/50000], Loss: 0.7187, Acc: 0.82\n",
      "Step [2500/50000], Loss: 0.6491, Acc: 0.84\n",
      "Step [2600/50000], Loss: 0.5564, Acc: 0.89\n",
      "Step [2700/50000], Loss: 0.5521, Acc: 0.91\n",
      "Step [2800/50000], Loss: 0.4167, Acc: 0.91\n",
      "Step [2900/50000], Loss: 0.6249, Acc: 0.84\n",
      "Step [3000/50000], Loss: 0.6135, Acc: 0.85\n",
      "Step [3100/50000], Loss: 0.4135, Acc: 0.92\n",
      "Step [3200/50000], Loss: 0.4989, Acc: 0.85\n",
      "Step [3300/50000], Loss: 0.4503, Acc: 0.90\n",
      "Step [3400/50000], Loss: 0.5291, Acc: 0.86\n",
      "Step [3500/50000], Loss: 0.4235, Acc: 0.94\n",
      "Step [3600/50000], Loss: 0.4275, Acc: 0.89\n",
      "Step [3700/50000], Loss: 0.2658, Acc: 0.97\n",
      "Step [3800/50000], Loss: 0.4965, Acc: 0.91\n",
      "Step [3900/50000], Loss: 0.3982, Acc: 0.90\n",
      "Step [4000/50000], Loss: 0.3856, Acc: 0.91\n",
      "Step [4100/50000], Loss: 0.4737, Acc: 0.87\n",
      "Step [4200/50000], Loss: 0.4339, Acc: 0.89\n",
      "Step [4300/50000], Loss: 0.5335, Acc: 0.85\n",
      "Step [4400/50000], Loss: 0.3484, Acc: 0.93\n",
      "Step [4500/50000], Loss: 0.3894, Acc: 0.90\n",
      "Step [4600/50000], Loss: 0.4011, Acc: 0.90\n",
      "Step [4700/50000], Loss: 0.4052, Acc: 0.92\n",
      "Step [4800/50000], Loss: 0.5132, Acc: 0.85\n",
      "Step [4900/50000], Loss: 0.3612, Acc: 0.90\n",
      "Step [5000/50000], Loss: 0.2680, Acc: 0.94\n",
      "Step [5100/50000], Loss: 0.2916, Acc: 0.95\n",
      "Step [5200/50000], Loss: 0.2882, Acc: 0.91\n",
      "Step [5300/50000], Loss: 0.3169, Acc: 0.91\n",
      "Step [5400/50000], Loss: 0.2635, Acc: 0.95\n",
      "Step [5500/50000], Loss: 0.3680, Acc: 0.90\n",
      "Step [5600/50000], Loss: 0.2441, Acc: 0.97\n",
      "Step [5700/50000], Loss: 0.4133, Acc: 0.91\n",
      "Step [5800/50000], Loss: 0.5006, Acc: 0.84\n",
      "Step [5900/50000], Loss: 0.3086, Acc: 0.91\n",
      "Step [6000/50000], Loss: 0.3980, Acc: 0.85\n",
      "Step [6100/50000], Loss: 0.4821, Acc: 0.90\n",
      "Step [6200/50000], Loss: 0.4143, Acc: 0.89\n",
      "Step [6300/50000], Loss: 0.2916, Acc: 0.91\n",
      "Step [6400/50000], Loss: 0.3963, Acc: 0.90\n",
      "Step [6500/50000], Loss: 0.4050, Acc: 0.92\n",
      "Step [6600/50000], Loss: 0.3174, Acc: 0.89\n",
      "Step [6700/50000], Loss: 0.3485, Acc: 0.89\n",
      "Step [6800/50000], Loss: 0.4549, Acc: 0.89\n",
      "Step [6900/50000], Loss: 0.2763, Acc: 0.94\n",
      "Step [7000/50000], Loss: 0.2954, Acc: 0.94\n",
      "Step [7100/50000], Loss: 0.2350, Acc: 0.96\n",
      "Step [7200/50000], Loss: 0.2424, Acc: 0.92\n",
      "Step [7300/50000], Loss: 0.3120, Acc: 0.90\n",
      "Step [7400/50000], Loss: 0.3258, Acc: 0.92\n",
      "Step [7500/50000], Loss: 0.2395, Acc: 0.94\n",
      "Step [7600/50000], Loss: 0.4438, Acc: 0.86\n",
      "Step [7700/50000], Loss: 0.4505, Acc: 0.89\n",
      "Step [7800/50000], Loss: 0.3919, Acc: 0.93\n",
      "Step [7900/50000], Loss: 0.2545, Acc: 0.91\n",
      "Step [8000/50000], Loss: 0.3089, Acc: 0.95\n",
      "Step [8100/50000], Loss: 0.2890, Acc: 0.94\n",
      "Step [8200/50000], Loss: 0.3771, Acc: 0.91\n",
      "Step [8300/50000], Loss: 0.2326, Acc: 0.94\n",
      "Step [8400/50000], Loss: 0.3399, Acc: 0.92\n",
      "Step [8500/50000], Loss: 0.2424, Acc: 0.95\n",
      "Step [8600/50000], Loss: 0.2784, Acc: 0.91\n",
      "Step [8700/50000], Loss: 0.4833, Acc: 0.88\n",
      "Step [8800/50000], Loss: 0.3119, Acc: 0.91\n",
      "Step [8900/50000], Loss: 0.2499, Acc: 0.90\n",
      "Step [9000/50000], Loss: 0.2201, Acc: 0.98\n",
      "Step [9100/50000], Loss: 0.3281, Acc: 0.92\n",
      "Step [9200/50000], Loss: 0.2667, Acc: 0.96\n",
      "Step [9300/50000], Loss: 0.2570, Acc: 0.95\n",
      "Step [9400/50000], Loss: 0.4090, Acc: 0.90\n",
      "Step [9500/50000], Loss: 0.3025, Acc: 0.91\n",
      "Step [9600/50000], Loss: 0.2683, Acc: 0.92\n",
      "Step [9700/50000], Loss: 0.3047, Acc: 0.92\n",
      "Step [9800/50000], Loss: 0.2733, Acc: 0.90\n",
      "Step [9900/50000], Loss: 0.2445, Acc: 0.94\n",
      "Step [10000/50000], Loss: 0.3052, Acc: 0.90\n",
      "Step [10100/50000], Loss: 0.4078, Acc: 0.88\n",
      "Step [10200/50000], Loss: 0.1669, Acc: 0.93\n",
      "Step [10300/50000], Loss: 0.3814, Acc: 0.91\n",
      "Step [10400/50000], Loss: 0.3374, Acc: 0.88\n",
      "Step [10500/50000], Loss: 0.2534, Acc: 0.92\n",
      "Step [10600/50000], Loss: 0.2586, Acc: 0.95\n",
      "Step [10700/50000], Loss: 0.2430, Acc: 0.95\n",
      "Step [10800/50000], Loss: 0.2131, Acc: 0.94\n",
      "Step [10900/50000], Loss: 0.2781, Acc: 0.94\n",
      "Step [11000/50000], Loss: 0.2852, Acc: 0.90\n",
      "Step [11100/50000], Loss: 0.2880, Acc: 0.93\n",
      "Step [11200/50000], Loss: 0.3149, Acc: 0.94\n",
      "Step [11300/50000], Loss: 0.4662, Acc: 0.92\n",
      "Step [11400/50000], Loss: 0.3065, Acc: 0.92\n",
      "Step [11500/50000], Loss: 0.2420, Acc: 0.91\n",
      "Step [11600/50000], Loss: 0.2181, Acc: 0.94\n",
      "Step [11700/50000], Loss: 0.2904, Acc: 0.93\n",
      "Step [11800/50000], Loss: 0.3374, Acc: 0.92\n",
      "Step [11900/50000], Loss: 0.3457, Acc: 0.91\n",
      "Step [12000/50000], Loss: 0.2624, Acc: 0.92\n",
      "Step [12100/50000], Loss: 0.2497, Acc: 0.93\n",
      "Step [12200/50000], Loss: 0.2761, Acc: 0.92\n",
      "Step [12300/50000], Loss: 0.4120, Acc: 0.91\n",
      "Step [12400/50000], Loss: 0.4992, Acc: 0.89\n",
      "Step [12500/50000], Loss: 0.2212, Acc: 0.95\n",
      "Step [12600/50000], Loss: 0.1798, Acc: 0.96\n",
      "Step [12700/50000], Loss: 0.1915, Acc: 0.95\n",
      "Step [12800/50000], Loss: 0.2465, Acc: 0.95\n",
      "Step [12900/50000], Loss: 0.2510, Acc: 0.93\n",
      "Step [13000/50000], Loss: 0.2499, Acc: 0.92\n",
      "Step [13100/50000], Loss: 0.2414, Acc: 0.95\n",
      "Step [13200/50000], Loss: 0.2438, Acc: 0.92\n",
      "Step [13300/50000], Loss: 0.2321, Acc: 0.94\n",
      "Step [13400/50000], Loss: 0.3417, Acc: 0.92\n",
      "Step [13500/50000], Loss: 0.2738, Acc: 0.93\n",
      "Step [13600/50000], Loss: 0.3740, Acc: 0.89\n",
      "Step [13700/50000], Loss: 0.2707, Acc: 0.90\n",
      "Step [13800/50000], Loss: 0.2830, Acc: 0.96\n",
      "Step [13900/50000], Loss: 0.3110, Acc: 0.91\n",
      "Step [14000/50000], Loss: 0.3298, Acc: 0.90\n",
      "Step [14100/50000], Loss: 0.4692, Acc: 0.89\n",
      "Step [14200/50000], Loss: 0.3140, Acc: 0.93\n",
      "Step [14300/50000], Loss: 0.3139, Acc: 0.92\n",
      "Step [14400/50000], Loss: 0.3285, Acc: 0.91\n",
      "Step [14500/50000], Loss: 0.2492, Acc: 0.93\n",
      "Step [14600/50000], Loss: 0.2556, Acc: 0.92\n",
      "Step [14700/50000], Loss: 0.3358, Acc: 0.92\n",
      "Step [14800/50000], Loss: 0.2448, Acc: 0.94\n",
      "Step [14900/50000], Loss: 0.2678, Acc: 0.94\n",
      "Step [15000/50000], Loss: 0.2890, Acc: 0.94\n",
      "Step [15100/50000], Loss: 0.1546, Acc: 0.97\n",
      "Step [15200/50000], Loss: 0.2023, Acc: 0.94\n",
      "Step [15300/50000], Loss: 0.2954, Acc: 0.93\n",
      "Step [15400/50000], Loss: 0.4778, Acc: 0.87\n",
      "Step [15500/50000], Loss: 0.2604, Acc: 0.94\n",
      "Step [15600/50000], Loss: 0.2001, Acc: 0.94\n",
      "Step [15700/50000], Loss: 0.2212, Acc: 0.94\n",
      "Step [15800/50000], Loss: 0.2751, Acc: 0.94\n",
      "Step [15900/50000], Loss: 0.2682, Acc: 0.93\n",
      "Step [16000/50000], Loss: 0.1187, Acc: 0.98\n",
      "Step [16100/50000], Loss: 0.3018, Acc: 0.87\n",
      "Step [16200/50000], Loss: 0.1999, Acc: 0.93\n",
      "Step [16300/50000], Loss: 0.2464, Acc: 0.91\n",
      "Step [16400/50000], Loss: 0.3710, Acc: 0.90\n",
      "Step [16500/50000], Loss: 0.3071, Acc: 0.95\n",
      "Step [16600/50000], Loss: 0.2697, Acc: 0.90\n",
      "Step [16700/50000], Loss: 0.3230, Acc: 0.91\n",
      "Step [16800/50000], Loss: 0.1594, Acc: 0.95\n",
      "Step [16900/50000], Loss: 0.2385, Acc: 0.93\n",
      "Step [17000/50000], Loss: 0.1851, Acc: 0.93\n",
      "Step [17100/50000], Loss: 0.1741, Acc: 0.95\n",
      "Step [17200/50000], Loss: 0.2484, Acc: 0.89\n",
      "Step [17300/50000], Loss: 0.2075, Acc: 0.94\n",
      "Step [17400/50000], Loss: 0.3010, Acc: 0.93\n",
      "Step [17500/50000], Loss: 0.2574, Acc: 0.88\n",
      "Step [17600/50000], Loss: 0.1929, Acc: 0.96\n",
      "Step [17700/50000], Loss: 0.2872, Acc: 0.92\n",
      "Step [17800/50000], Loss: 0.2347, Acc: 0.92\n",
      "Step [17900/50000], Loss: 0.1646, Acc: 0.95\n",
      "Step [18000/50000], Loss: 0.2375, Acc: 0.93\n",
      "Step [18100/50000], Loss: 0.2671, Acc: 0.95\n",
      "Step [18200/50000], Loss: 0.2138, Acc: 0.93\n",
      "Step [18300/50000], Loss: 0.1173, Acc: 0.99\n",
      "Step [18400/50000], Loss: 0.2096, Acc: 0.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-87e43819402e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# Fetch images and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# %load main.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from logger import Logger\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# MNIST dataset \n",
    "dataset = torchvision.datasets.MNIST(root='../../data', \n",
    "                                     train=True, \n",
    "                                     transform=transforms.ToTensor(),  \n",
    "                                     download=True)\n",
    "\n",
    "# Data loader\n",
    "data_loader = torch.utils.data.DataLoader(dataset=dataset, \n",
    "                                          batch_size=100, \n",
    "                                          shuffle=True)\n",
    "\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_size=500, num_classes=10):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet().to(device)\n",
    "\n",
    "logger = Logger('./logs')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.00001)  \n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "iter_per_epoch = len(data_loader)\n",
    "total_step = 50000\n",
    "\n",
    "# Start training\n",
    "for step in range(total_step):\n",
    "    \n",
    "    # Reset the data_iter\n",
    "    if (step+1) % iter_per_epoch == 0:\n",
    "        data_iter = iter(data_loader)\n",
    "\n",
    "    # Fetch images and labels\n",
    "    images, labels = next(data_iter)\n",
    "    images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Compute accuracy\n",
    "    _, argmax = torch.max(outputs, 1)\n",
    "    accuracy = (labels == argmax.squeeze()).float().mean()\n",
    "\n",
    "    if (step+1) % 100 == 0:\n",
    "        print ('Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
    "               .format(step+1, total_step, loss.item(), accuracy.item()))\n",
    "\n",
    "        # ================================================================== #\n",
    "        #                        Tensorboard Logging                         #\n",
    "        # ================================================================== #\n",
    "\n",
    "        # 1. Log scalar values (scalar summary)\n",
    "        info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
    "\n",
    "        for tag, value in info.items():\n",
    "            logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "        # 2. Log values and gradients of the parameters (histogram summary)\n",
    "        for tag, value in model.named_parameters():\n",
    "            tag = tag.replace('.', '/')\n",
    "            logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
    "            logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), step+1)\n",
    "\n",
    "        # 3. Log training images (image summary)\n",
    "        info = { 'images': images.view(-1, 28, 28)[:10].cpu().numpy() }\n",
    "\n",
    "        for tag, images in info.items():\n",
    "            logger.image_summary(tag, images, step+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results go into even.out.tfeventfiles withing the logs folder uner the current folder. \n",
    "To run the tensorbpard the command is: \n",
    "\n",
    "$ ``tensorboard --logdir .\\logs --host localhost''\n",
    "\n",
    "To see the tensorboard in the browser: ``tp://localhost:6006``\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
